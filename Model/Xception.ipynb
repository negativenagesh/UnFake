{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/org/science/article/pii/S154622182400835X#s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, image\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths to your dataset folders (update these paths as per your system)\n",
    "real_images_path = 'path/to/Real'\n",
    "fake_images_path = 'path/to/Fake'\n",
    "\n",
    "# Collect image paths and labels (0 for real, 1 for fake)\n",
    "real_images = [os.path.join(real_images_path, f) for f in os.listdir(real_images_path) \n",
    "               if f.lower().endswith(('.jpg', '.png'))]\n",
    "fake_images = [os.path.join(fake_images_path, f) for f in os.listdir(fake_images_path) \n",
    "               if f.lower().endswith(('.jpg', '.png'))]\n",
    "all_images = real_images + fake_images\n",
    "all_labels = [0] * len(real_images) + [1] * len(fake_images)\n",
    "\n",
    "print(f\"Total images: {len(all_images)} (Real: {len(real_images)}, Fake: {len(fake_images)})\")\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataframes for train and validation\n",
    "train_df = pd.DataFrame({'filename': train_images, 'class': train_labels})\n",
    "val_df = pd.DataFrame({'filename': val_images, 'class': val_labels})\n",
    "train_df['class'] = train_df['class'].astype(str)\n",
    "val_df['class'] = val_df['class'].astype(str)\n",
    "\n",
    "# Define image size and batch size\n",
    "img_height, img_width = 299, 299  # Xception input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Preprocessing for validation (no augmentation)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Build the Xception-based model\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the base model layers initially\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model_initial.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# Initial training\n",
    "print(\"Starting initial training with frozen base layers...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze the last 10 layers\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Update callbacks for fine-tuning\n",
    "callbacks[-1] = ModelCheckpoint('best_model_finetuned.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Fine-tuning training\n",
    "print(\"Starting fine-tuning with unfrozen layers...\")\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save('deepfake_detector_final.h5')\n",
    "\n",
    "# Prediction function\n",
    "def predict_image(img_path, model):\n",
    "    \"\"\"Predict whether an image is Real or Fake.\"\"\"\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        return 'Fake' if prediction[0] > 0.5 else 'Real'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "val_predictions = model.predict(validation_generator)\n",
    "val_pred_labels = (val_predictions > 0.5).astype(int)\n",
    "val_true_labels = validation_generator.classes\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_true_labels, val_pred_labels, target_names=['Real', 'Fake']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(val_true_labels, val_pred_labels))\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history, 'Initial Training')\n",
    "plot_history(history_fine, 'Fine-Tuning')\n",
    "\n",
    "# Example usage of prediction\n",
    "# test_image_path = 'path/to/test/image.jpg'\n",
    "# result = predict_image(test_image_path, model)\n",
    "# print(f\"Prediction for {test_image_path}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
